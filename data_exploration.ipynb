{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "import vjp.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1363f51b",
   "metadata": {},
   "source": [
    "## Data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_instance_samples = data.load_second_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ca6fa",
   "metadata": {},
   "source": [
    "Samples are parsed as Python XML element trees. Queries can be done per-element through [the XPath syntax](https://docs.python.org/3/library/xml.etree.elementtree.html#supported-xpath-syntax). `vjp.data.findall` permits querying an entire list. All results from all queries are added to an output list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cae14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of instances', len(second_instance_samples))\n",
    "print('Number of second instance requests',\n",
    "      len(data.findall(second_instance_samples, \".//partreq[@G='2']/req\")[0]))\n",
    "\n",
    "decisions, decision_mapping = data.findall(second_instance_samples,\n",
    "                                           \".//courtdec[@G='2']/dec\")\n",
    "upheld, _ = data.findall(decisions, \".[@E='1']\")\n",
    "rejected, _ = data.findall(decisions, \".[@E='0']\")\n",
    "print('Number of second instance decisions', len(decisions))\n",
    "print('Of which upheld', len(upheld))\n",
    "print('Of which rejected', len(rejected))\n",
    "print('Other outcomes', len(decisions) - len(upheld) - len(rejected))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f35153",
   "metadata": {},
   "source": [
    "Exploring requests IDS, claims and arguments to get a full picture of their interconnections shows that some values can be `None` and tagnames are not always consistent. Malformed statements could maybe be manually repaired, but they will be simply ignored for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f48411",
   "metadata": {},
   "outputs": [],
   "source": [
    "reqs, _ = data.findall(second_instance_samples,\n",
    "                       \".//partreq[@G='2']/req\")\n",
    "print('Request IDs', set(req.get('ID') for req in reqs), '\\n')\n",
    "\n",
    "claims, _ = data.findall(second_instance_samples,\n",
    "                         \".//partreq[@G='2']/claim\")\n",
    "print('Claim PROs', set(claim.get('PRO') for claim in claims), '\\n')\n",
    "\n",
    "args, _ = data.findall(second_instance_samples,\n",
    "                       \".//partreq[@G='2']/arg\")\n",
    "print('Arg PROs', set(arg.get('PRO') for arg in args), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebffefae",
   "metadata": {},
   "source": [
    "Some decisions reference multiple requests. A decision that implies the result of multiple requests may be later splitted.\n",
    "\n",
    "Some decisions reference claims. This shall be investigated (do they imply a label for some given requests?). The number is relatively small, they could be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = tuple(map(lambda x: x.get('O'), decisions))\n",
    "print(set(objects))\n",
    "print('Number of claim objects: ',\n",
    "      len(tuple(filter(lambda s: not s.startswith('Req'), objects))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3585684",
   "metadata": {},
   "source": [
    "Given the very low amount of different outcomes, the problem will be treated as a binary classification one. Labels are not perfectly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fca7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tuple(map(lambda x: x.get('E'), decisions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7706cc6a",
   "metadata": {},
   "source": [
    "Other outcomes can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d9c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = upheld + rejected\n",
    "second_instance_samples = list(set(decision_mapping[decision]\n",
    "                                   for decision in decisions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ffb50",
   "metadata": {},
   "source": [
    "Data will need some processing and handling of special cases. Some values can be `None` (empty tags?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for decision in decisions[:10]:\n",
    "    print(decision.text, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35caf5",
   "metadata": {},
   "source": [
    "Some elements have multiple links. It is particularly interesting when it happens in decision tags, as each decision-request pair can form a new data sample. Iterative filtering and extraction of multiple links makes it possible to explore the whole \"tree\" of connections between nodes in the document, starting from decision tags and ending wherever it is decided.\n",
    "A minimal set of features would be: request, arguments and claim tags, labeled by the corresponding decision tag, as defined by [Galli et. al, 2022]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c821aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multiple_link_decisions = tuple(filter(lambda e: '|' in e.get('O'), decisions))\n",
    "print('Number of multilinked decisions', len(multiple_link_decisions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d21a2",
   "metadata": {},
   "source": [
    "## Graph tag representation\n",
    "\n",
    "To facilitate the task of composing different feature sets based on how they are linked, each document is flattened into a set of triples, then used to build a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c3538",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_dfs = [data.build_tag_triples(sample)\n",
    "               for sample in second_instance_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08702b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(triples_dfs[0].shape)\n",
    "triples_dfs[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81194d13",
   "metadata": {},
   "source": [
    "A rapid check for null values (the builder function comes without warranty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e967f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null_list = [df.isnull().values.any() for df in triples_dfs]\n",
    "print(any(check_null_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c483bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = [nx.from_pandas_edgelist(triples, edge_attr='edge',\n",
    "                                  create_using=nx.DiGraph())\n",
    "          for triples in triples_dfs]\n",
    "len(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graphs[2].copy()\n",
    "\n",
    "# Remove initial uninformative components\n",
    "# for component in tuple(nx.connected_components(graph.to_undirected())):\n",
    "#     if len(component) <= 2:\n",
    "#         graph.remove_nodes_from(tuple(component))\n",
    "\n",
    "# Remove finds for a cleaner representation\n",
    "for node in tuple(graph.nodes):\n",
    "    if node.lower().startswith('find'):\n",
    "        graph.remove_node(node)\n",
    "\n",
    "print('Connected components',\n",
    "      tuple(nx.connected_components(graph.to_undirected())))\n",
    "\n",
    "graph_pos = nx.planar_layout(graph)\n",
    "nx.draw(graph, pos=graph_pos, node_size=0, font_size=10, with_labels=True,\n",
    "        arrowsize=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed3038",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'req', 'arg', 'claim'\n",
    "df_list = []\n",
    "\n",
    "# graphs = graphs[3:5]\n",
    "# second_instance_samples = second_instance_samples[3:5]\n",
    "\n",
    "for graph, document in zip(graphs, second_instance_samples):\n",
    "    for component in tuple(nx.connected_components(graph.to_undirected())):\n",
    "        take = False\n",
    "        label = -1\n",
    "        for node in component:\n",
    "            if node.lower().startswith('dec'):\n",
    "                dec = document.find(f\".//*[@ID='{node}']\")\n",
    "\n",
    "                try:\n",
    "                    label = int(dec.get('E'))\n",
    "                    if label not in (0, 1):\n",
    "                        raise ValueError\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                    \n",
    "                take = int(dec.get('G')) == 2\n",
    "                break\n",
    "\n",
    "        if not take:\n",
    "            continue\n",
    "        \n",
    "        concat_lists = [[] for _ in PREFIX]\n",
    "        for node in component:\n",
    "            for i, prefix in enumerate(PREFIX):\n",
    "                if node.lower().startswith(prefix):\n",
    "                    node_element = document.find(f\".//*[@ID='{node}']\")\n",
    "                    if node_element.text is not None:\n",
    "                        concat_lists[i].append(node_element.text)\n",
    "\n",
    "        fact_element = document.find(f\".//fact\")\n",
    "        fact = ''\n",
    "        if fact_element is not None:\n",
    "            fact = fact_element.text\n",
    "\n",
    "        req_prefix_index = PREFIX.index('req')\n",
    "        for req_text in concat_lists[req_prefix_index]:\n",
    "            df_list.append([fact, *map(' '.join, concat_lists), label])\n",
    "            df_list[-1][1 + req_prefix_index] = req_text\n",
    "\n",
    "df = pd.DataFrame(df_list, columns=['fact', *PREFIX, 'label'])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72923050",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "\n",
    "[Galli et. al, 2022]: Galli, F., Grundler, G., Fidelangeli, A., Galassi, A., Lagioia, F., Palmieri, E., Ruggeri, F., Sartor, G., & Torroni, P. (2022). Predicting outcomes of Italian VAT DECISIONS1. *Frontiers in Artificial Intelligence and Applications*. https://doi.org/10.3233/faia220465   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VJP",
   "language": "python",
   "name": "vjp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
