{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d74d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import vjp.data as data\n",
    "import vjp.preprocess as preprocess\n",
    "import vjp.folds as folds\n",
    "import vjp.text as text\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import vjp.preprocess as preprocess\n",
    "import vjp.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadce0b3",
   "metadata": {},
   "source": [
    "Retrieve a preprocessed dataframe using the pipeline defined by `data_exploration.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9103a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file if exists (e.g. pregenerated via preprocess CLI)\n",
    "DF_FILENAME = 'connected_components.parquet'\n",
    "if os.path.exists(DF_FILENAME):\n",
    "    print(f'Reading from {DF_FILENAME}...')\n",
    "    df = pd.read_parquet(DF_FILENAME)\n",
    "else:           # Generate on the go\n",
    "    print('File not found, generating dataframe...')\n",
    "    namespace = preprocess.Namespace()\n",
    "    namespace.connected_component_tags = ('req', 'arg', 'claim', 'mot', 'dec')\n",
    "    namespace.use_child_text_tag_names = ('mot', 'dec')\n",
    "    namespace.level = preprocess.PreprocessingLevels.CONNECTED_COMPONENTS\n",
    "    df = preprocess.preprocess(namespace)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "random_state = 1717"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee26e5",
   "metadata": {},
   "source": [
    "## Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492227dc",
   "metadata": {},
   "source": [
    "All tag types are gathered, so that multiple experiments may be carried out by eventually excluding some of the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc841c",
   "metadata": {},
   "source": [
    "Balanced KFold splits are computed at document level using a MIP formulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_maps = folds.compute_decision_folds(df, verbose=True, seed=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d867a6e",
   "metadata": {},
   "source": [
    "Preview of the first split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[fold_maps[0]].shape)\n",
    "df[fold_maps[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0f5f6",
   "metadata": {},
   "source": [
    "A `split` function encapsules all the kfold logic and provides train-test splits based on the its results. The function has a similar interface to the one of scikit-learn's validators, and is suitable to be used with its crossvalidation based metrics (`GridSearchCV`, `cross_validate`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83193310",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_indeces, test_indeces in folds.split(df):\n",
    "    print(len(train_indeces), len(test_indeces))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ed489",
   "metadata": {},
   "source": [
    "## Count based encodings\n",
    "For count based encodings (e.g. tf-idf) text data shall be cleaned in a certain way. Punctuations and symbols, most stopwords, etc. are not required, as the order and structure of sentences is generally lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.load_stopwords()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802ea41",
   "metadata": {},
   "source": [
    "`vjp.text` contains some pipelines that: lower text, remove punctuation, remove stopwords and lemmatize. Such transformations can be applied to the desired features before feeding them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7decd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['fact', 'req', 'arg', 'claim', 'mot', 'dec']\n",
    "\n",
    "# Lemmatize but keep unknown values\n",
    "df_keep = df.copy()\n",
    "df_keep[tags] = df[tags].applymap(text.count_keep_pipeline)\n",
    "\n",
    "# Lemmatize but drop unknown values\n",
    "df_drop = df.copy()\n",
    "df_drop[tags] = df[tags].applymap(text.count_drop_pipeline)\n",
    "\n",
    "# Don't lemmatize\n",
    "df_no_lem = df.copy()\n",
    "df_no_lem[tags] = df_no_lem[tags].applymap(text.count_pipeline_head)\n",
    "\n",
    "dataframes = ('keep', df_keep), ('drop', df_drop), ('no_lem', df_no_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbe070e",
   "metadata": {},
   "source": [
    "Features will be concatenated for easier vectorization and labels are splitted as demanded by `sklearn` models via `data.count_based_X_y`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebde989",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d64081",
   "metadata": {},
   "source": [
    "A simple dummy baseline is defined (defaults to majority class). Models are evaluated on three different preprocessing pipelines in order to inspect the effect of lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f760b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_baseline = DummyClassifier(random_state=random_state)\n",
    "\n",
    "# {df_name: (mean, variance), ...}\n",
    "results = {}\n",
    "for name, df in dataframes:\n",
    "    dummy_results = models.cross_validate(dummy_baseline, df,\n",
    "                                          cv=folds.split(df))\n",
    "    results[name] = dummy_results.mean(), dummy_results.std()\n",
    "\n",
    "pd.DataFrame(results, ('mean', 'std'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88bd19",
   "metadata": {},
   "source": [
    "A random forest and a linear SVC are built. We focus on these two models as they provide some form of interpretability of the features' weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = Pipeline([('vectorizer', TfidfVectorizer()),\n",
    "                          ('model', RandomForestClassifier(\n",
    "                              random_state=random_state))])\n",
    "\n",
    "results = {}\n",
    "for name, df in dataframes:\n",
    "    forest_results = models.cross_validate(random_forest, df,\n",
    "                                           cv=folds.split(df))\n",
    "    results[name] = forest_results.mean(), forest_results.std()\n",
    "\n",
    "pd.DataFrame(results, ('mean', 'std'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd308d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = Pipeline([('vectorizer', TfidfVectorizer()),\n",
    "                       ('model', LinearSVC(random_state=random_state))])\n",
    "results = {}\n",
    "for name, df in dataframes:\n",
    "    svc_results = models.cross_validate(linear_svc, df,\n",
    "                                        cv=folds.split(df))\n",
    "    results[name] = svc_results.mean(), svc_results.std()\n",
    "\n",
    "pd.DataFrame(results, ('mean', 'std'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b5f59",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "In an effort to interpret model results, Gini importance values are extracted from a random forest and weights are extracted from a SVC. For convenience (not having to deal with multiple splits) the models are fit on the whole dataset. The performance of such models are not evaluated (how could it be?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = random_forest.fit(*data.count_based_X_y(df_no_lem,\n",
    "                                                        models.DEFAULT_TAGS))\n",
    "importances = pd.Series(random_forest[-1].feature_importances_,\n",
    "                        random_forest[0].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce67f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Gini importance')\n",
    "plt.title('Random forest feature importance')\n",
    "plt.bar(*zip(*importances.sort_values(ascending=False)[:30].items()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca6adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = linear_svc.fit(*data.count_based_X_y(df_no_lem,\n",
    "                                                  models.DEFAULT_TAGS))\n",
    "coefficients = pd.Series(linear_svc[-1].coef_[0],\n",
    "                         linear_svc[0].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('$|w_i|$')\n",
    "plt.title('Linear SVC Best weights')\n",
    "plt.bar(*zip(*abs(coefficients).sort_values(ascending=False)[:30].items()))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VJP",
   "language": "python",
   "name": "vjp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "44c8d51243964ac018940b6314270b9dce7e7671e647e320692ad953ee52b48e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
